# data/data_cache.py
"""
æœ¬åœ°æ•°æ®ç¼“å­˜
ä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼Œå‡å°‘APIè°ƒç”¨
"""
import pandas as pd
import os
import json
from datetime import datetime
from pathlib import Path


class DataCache:
    """æœ¬åœ°æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir='data_cache'):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Parameters:
        -----------
        cache_dir : str
            ç¼“å­˜ç›®å½•è·¯å¾„
        """
        self.cache_dir = Path(cache_dir)
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if not self.cache_dir.exists():
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            print(f"âœ… åˆ›å»ºç¼“å­˜ç›®å½•: {self.cache_dir}")
        
        # å…ƒæ•°æ®æ–‡ä»¶
        self.metadata_file = self.cache_dir / 'metadata.json'
        self.metadata = self._load_metadata()
    
    def _load_metadata(self):
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸  åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
                return {}
        return {}
    
    def _save_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def get_cache_path(self, stock_code, start_date, end_date):
        """
        ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        Parameters:
        -----------
        stock_code : str
            è‚¡ç¥¨ä»£ç 
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        
        Returns:
        --------
        Path : ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        # æ¸…ç†è‚¡ç¥¨ä»£ç ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        clean_code = stock_code.replace('.', '_')
        filename = f"{clean_code}_{start_date}_{end_date}.csv"
        return self.cache_dir / filename
    
    def load(self, stock_code, start_date, end_date):
        """
        åŠ è½½ç¼“å­˜æ•°æ®
        
        Parameters:
        -----------
        stock_code : str
            è‚¡ç¥¨ä»£ç 
        start_date : str
            å¼€å§‹æ—¥æœŸ 'YYYY-MM-DD'
        end_date : str
            ç»“æŸæ—¥æœŸ 'YYYY-MM-DD'
        
        Returns:
        --------
        DataFrame or None : ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        cache_path = self.get_cache_path(stock_code, start_date, end_date)
        cache_key = f"{stock_code}_{start_date}_{end_date}"
        
        if not cache_path.exists():
            return None
        
        try:
            # è¯»å–CSVï¼ˆä¸ä½¿ç”¨ç´¢å¼•ï¼‰
            df = pd.read_csv(cache_path)
            
            # éªŒè¯æ•°æ®æ ¼å¼
            required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']
            if not all(col in df.columns for col in required_cols):
                print(f"âš ï¸  ç¼“å­˜æ•°æ®æ ¼å¼é”™è¯¯ï¼ˆç¼ºå°‘å¿…éœ€åˆ—ï¼‰: {cache_key}")
                return None
            
            # ç¡®ä¿ date åˆ—æ˜¯å­—ç¬¦ä¸²æ ¼å¼
            if df['date'].dtype != 'object':
                df['date'] = df['date'].astype(str)
            
            # è·å–ç¼“å­˜æ—¶é—´
            if cache_key in self.metadata:
                cache_time = self.metadata[cache_key].get('cached_at', '')
                print(f"ğŸ“ [Cache] ä½¿ç”¨ç¼“å­˜: {stock_code} ({len(df)}è¡Œ) [ç¼“å­˜äº {cache_time}]")
            else:
                print(f"ğŸ“ [Cache] ä½¿ç”¨ç¼“å­˜: {stock_code} ({len(df)}è¡Œ)")
            
            return df
            
        except Exception as e:
            print(f"âš ï¸  è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            # åˆ é™¤æŸåçš„ç¼“å­˜
            try:
                cache_path.unlink()
                if cache_key in self.metadata:
                    del self.metadata[cache_key]
                    self._save_metadata()
            except:
                pass
            return None
    
    def save(self, stock_code, start_date, end_date, df):
        """
        ä¿å­˜æ•°æ®åˆ°ç¼“å­˜
        
        Parameters:
        -----------
        stock_code : str
            è‚¡ç¥¨ä»£ç 
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        df : DataFrame
            æ•°æ®
        """
        if df is None or len(df) == 0:
            return
        
        try:
            cache_path = self.get_cache_path(stock_code, start_date, end_date)
            cache_key = f"{stock_code}_{start_date}_{end_date}"
            
            # éªŒè¯æ•°æ®æ ¼å¼
            if 'date' not in df.columns:
                print(f"âš ï¸  æ•°æ®ç¼ºå°‘dateåˆ—ï¼Œæ— æ³•ç¼“å­˜")
                return
            
            # ä¿å­˜CSVï¼ˆä¸ä½¿ç”¨ç´¢å¼•ï¼‰
            df.to_csv(cache_path, index=False)
            
            # æ›´æ–°å…ƒæ•°æ®
            self.metadata[cache_key] = {
                'stock_code': stock_code,
                'start_date': start_date,
                'end_date': end_date,
                'rows': len(df),
                'cached_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'file_path': str(cache_path)
            }
            self._save_metadata()
            
            print(f"ğŸ’¾ [Cache] å·²ç¼“å­˜: {stock_code} ({len(df)}è¡Œ) â†’ {cache_path.name}")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def clear_cache(self, stock_code=None):
        """
        æ¸…é™¤ç¼“å­˜
        
        Parameters:
        -----------
        stock_code : str, optional
            å¦‚æœæŒ‡å®šï¼Œåªæ¸…é™¤è¯¥è‚¡ç¥¨çš„ç¼“å­˜ï¼›å¦åˆ™æ¸…é™¤æ‰€æœ‰
        """
        if stock_code:
            # æ¸…é™¤ç‰¹å®šè‚¡ç¥¨çš„ç¼“å­˜
            removed = 0
            for cache_key in list(self.metadata.keys()):
                if self.metadata[cache_key]['stock_code'] == stock_code:
                    file_path = Path(self.metadata[cache_key]['file_path'])
                    if file_path.exists():
                        file_path.unlink()
                    del self.metadata[cache_key]
                    removed += 1
            
            if removed > 0:
                self._save_metadata()
                print(f"âœ… å·²æ¸…é™¤ {stock_code} çš„ {removed} ä¸ªç¼“å­˜")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ° {stock_code} çš„ç¼“å­˜")
        else:
            # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            count = 0
            for file in self.cache_dir.glob('*.csv'):
                file.unlink()
                count += 1
            
            self.metadata.clear()
            self._save_metadata()
            print(f"âœ… å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜ ({count} ä¸ªæ–‡ä»¶)")
    
    def list_cache(self):
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜"""
        if not self.metadata:
            print("ğŸ“­ ç¼“å­˜ä¸ºç©º")
            return
        
        print(f"\n{'='*80}")
        print(f"ç¼“å­˜åˆ—è¡¨ ({len(self.metadata)} é¡¹)")
        print(f"{'='*80}")
        print(f"{'è‚¡ç¥¨ä»£ç ':<15} {'æ—¥æœŸèŒƒå›´':<30} {'è¡Œæ•°':<8} {'ç¼“å­˜æ—¶é—´':<20}")
        print(f"{'-'*80}")
        
        for cache_key, info in sorted(self.metadata.items()):
            stock = info['stock_code']
            date_range = f"{info['start_date']} ~ {info['end_date']}"
            rows = info['rows']
            cached_at = info['cached_at']
            print(f"{stock:<15} {date_range:<30} {rows:<8} {cached_at:<20}")
        
        print(f"{'='*80}\n")
    
    def get_cache_size(self):
        """è·å–ç¼“å­˜æ€»å¤§å°"""
        total_size = 0
        for file in self.cache_dir.glob('*.csv'):
            total_size += file.stat().st_size
        
        # è½¬æ¢ä¸ºå¯è¯»æ ¼å¼
        if total_size < 1024:
            return f"{total_size} B"
        elif total_size < 1024 * 1024:
            return f"{total_size / 1024:.2f} KB"
        else:
            return f"{total_size / (1024 * 1024):.2f} MB"


# å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    print("\n" + "="*80)
    print("æ•°æ®ç¼“å­˜æµ‹è¯•")
    print("="*80)
    
    # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
    cache = DataCache()
    
    # æµ‹è¯•æ•°æ®
    test_data = pd.DataFrame({
        'date': ['2025-01-20', '2025-01-21', '2025-01-22'],
        'open': [100.0, 101.0, 102.0],
        'high': [105.0, 106.0, 107.0],
        'low': [99.0, 100.0, 101.0],
        'close': [103.0, 104.0, 105.0],
        'volume': [1000000, 1100000, 1200000]
    })
    
    # 1. ä¿å­˜ç¼“å­˜
    print("\nã€æµ‹è¯•1ã€‘ä¿å­˜ç¼“å­˜")
    print("-"*80)
    cache.save('TEST.STOCK', '2025-01-20', '2025-01-22', test_data)
    
    # 2. è¯»å–ç¼“å­˜
    print("\nã€æµ‹è¯•2ã€‘è¯»å–ç¼“å­˜")
    print("-"*80)
    cached = cache.load('TEST.STOCK', '2025-01-20', '2025-01-22')
    if cached is not None:
        print(f"âœ… è¯»å–æˆåŠŸ: {len(cached)} è¡Œ")
        print(cached)
    
    # 3. åˆ—å‡ºç¼“å­˜
    print("\nã€æµ‹è¯•3ã€‘åˆ—å‡ºæ‰€æœ‰ç¼“å­˜")
    cache.list_cache()
    
    # 4. ç¼“å­˜å¤§å°
    print(f"ã€æµ‹è¯•4ã€‘ç¼“å­˜å¤§å°: {cache.get_cache_size()}")
    
    # 5. æ¸…é™¤ç¼“å­˜
    print("\nã€æµ‹è¯•5ã€‘æ¸…é™¤æµ‹è¯•ç¼“å­˜")
    print("-"*80)
    cache.clear_cache('TEST.STOCK')
    
    print("\nâœ… æµ‹è¯•å®Œæˆ\n")
